from pyspark.sql import DataFrame
from spark_lineage.Exceptions import LinageException 
import inspect
import re
from functools import wraps


class Produced:
    def __init__(self):
        self.func = None 


def __check_spark_linage(df):
    if isinstance(df, DataFrame):
        has_sl = getattr(df, 'spark_linage', None)
        if not has_sl:
            raise LinageException('Dataframe is not integrated with spark linage. Please check if it is generated by a spark linage wrapper/decorator.')


def __check_arguments(*args, **kwargs):
    for arg in args:
        __check_spark_linage(arg)
    for key in kwargs.keys():
        __check_spark_linage(kwargs[key])


def produce(func):
    def _get_produced_columns(func):
        produced_cols = []
        s=inspect.getsource(func).replace(" ", "")
        with_columns = re.findall('colName=\'.*?\'',s)
        for produced_col in with_columns:
            produced_cols.append(produced_col[produced_col.find("\'")+1:-1])
        return produced_cols

    @wraps(func)
    def wrapper(*args, **kwargs):
        __check_arguments(*args, **kwargs)
        produced_columns = _get_produced_columns(func)
        df = func(*args, **kwargs)
        df.produced = produced_columns
        df.spark_linage = 'produce'
        return df
    return wrapper


def extract(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        __check_arguments(*args, **kwargs)
        alias = kwargs.get('alias')
        df = func(*args, **kwargs).alias(alias)
        df.context_alias = alias
        df.spark_linage = 'extract'
        return df
    return wrapper


def transform(func):
    pass


def enrich(func):
    pass